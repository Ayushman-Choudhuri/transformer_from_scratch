# Transformer Architecture for Nueral Machine Translation from Scratch

This repository is a implementation of the original **"Attention is all you need"** paper from **Google**. The motivation for this short side project was to gather a in-depth understanding of how various parts of the transformer architecture interacts with each other. No better way to understand this than deep diving and recreating the architecture. 

This understanding would further facilitate the understanding of more recent transformer based computer vision models such as `ViT`, `DETR`, `3DETR`, etc. 

## Contents


## 1. Theory


## 2. Setup


## 3. Training


## 4. Inferencing


## 5. Visualizations and Results



## References

[1] ["Attention is All You Need" - Original Paper ](https://arxiv.org/abs/1706.03762)

[2] ["Attention in transformers, visually explained" - 3blue1Brown](https://www.youtube.com/watch?v=eMlx5fFNoYc)


[3] ["The Annotated Transformer" - Harvard NLP](https://nlp.seas.harvard.edu/2018/04/03/attention.html)

[4] ["Transformer: A Novel Nueral Network Architecture for Language Understanding" - Google](https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/)

[6] ["The illustrated transformer" - Jay Alammar](https://jalammar.github.io/illustrated-transformer/)

[7] ["Illustrated Guide to Transformers Nueral Network" - The AI Hacker](https://www.youtube.com/watch?v=4Bdc55j80l8)

[8] ["How a transformer works at inference vs training time"](https://www.youtube.com/watch?v=IGu7ivuy1Ag)

[9.] ["Transformer architecture deepdive"](https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html)
